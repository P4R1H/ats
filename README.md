# ATS - Intelligent Applicant Tracking System

An ML-powered ATS that provides transparent, data-driven insights for both candidates and recruiters. Built using unsupervised learning, statistical analysis, and NLP.

**Academic Project** | Foundations of Data Science Course

---

## Features

### For Candidates
- ğŸ“Š **Multi-Factor Scoring**: Weighted algorithm (skills 40%, experience 30%, education 20%, bonuses 10%)
- ğŸ“ˆ **Percentile Ranking**: Know where you stand (e.g., "Top 25% of all candidates")
- ğŸ¯ **Skill Gap Analysis**: Identify missing skills with improvement recommendations
- ğŸ·ï¸ **Cluster Assignment**: See your candidate profile group

### For Recruiters
- ğŸ” **Smart Clustering**: K-means clustering groups candidates by skills/experience
- ğŸ“‹ **Top Candidates**: Filter by percentile bands (Top 10%, Top 25%, etc.)
- ğŸ¤ **Job Matching**: TF-IDF + cosine similarity for resume-job matching
- ğŸ“Š **Analytics**: Score distributions, skill frequencies, statistical insights

---

## Quick Start

### Prerequisites
- Python 3.9+
- pip (Python package manager)
- 4GB RAM minimum
- 2GB free disk space

### Installation (5-10 minutes)

```bash
# 1. Clone the repository
git clone <your-repo-url>
cd ATS

# 2. Set up Python environment
cd ml
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 4. Download NLP model
python -m spacy download en_core_web_sm
```

### Generate Data

```bash
# Generate 800+ synthetic resumes
python src/generate_synthetic_data.py

# When prompted, enter: 800
```

**Optional**: Download real resume dataset from Kaggle for better results:
1. Visit: https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset
2. Download CSV
3. Place in `ml/data/raw/resume_dataset.csv`
4. Notebooks will automatically combine both datasets

### Run the Analysis

```bash
# Start Jupyter Notebook
jupyter notebook

# Run notebooks in order:
# 1. 01_data_acquisition_and_exploration.ipynb
# 2. 02_data_preprocessing_and_cleaning.ipynb
# 3. 03_feature_engineering.ipynb
# 4. 04_clustering_analysis.ipynb
# 5. 05_scoring_and_ranking.ipynb
# 6. 06_statistical_validation.ipynb
```

**Important**: Run notebooks sequentially - each depends on outputs from previous ones.

### Expected Outputs

After running all notebooks, you should have:

```
ml/data/processed/
â”œâ”€â”€ resumes_cleaned.csv           # Cleaned data
â”œâ”€â”€ resumes_with_features.csv     # Engineered features
â”œâ”€â”€ resumes_clustered.csv         # Cluster assignments
â”œâ”€â”€ resumes_scored.csv            # Final scores & percentiles
â””â”€â”€ key_insights.txt              # Statistical insights

ml/models/
â”œâ”€â”€ kmeans_model.pkl              # Clustering model
â”œâ”€â”€ pca_model.pkl                 # Dimensionality reduction
â”œâ”€â”€ tfidf_vectorizer.pkl          # Text vectorizer
â”œâ”€â”€ feature_scaler.pkl            # Feature scaler
â””â”€â”€ [config files].json           # Model configurations
```

---

## Project Structure

```
ATS/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ 01_PROJECT_OVERVIEW.md    # High-level overview
â”‚   â”œâ”€â”€ 02_ML_METHODOLOGY.md      # FDS concepts & techniques
â”‚   â””â”€â”€ TROUBLESHOOTING.md        # Common issues & fixes
â”œâ”€â”€ ml/                            # Machine Learning Pipeline
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ raw/                  # Original datasets
â”‚   â”‚   â””â”€â”€ processed/            # Processed data
â”‚   â”œâ”€â”€ notebooks/                # Jupyter notebooks (01-06)
â”‚   â”‚   â”œâ”€â”€ 01_data_acquisition_and_exploration.ipynb
â”‚   â”‚   â”œâ”€â”€ 02_data_preprocessing_and_cleaning.ipynb
â”‚   â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚   â”‚   â”œâ”€â”€ 04_clustering_analysis.ipynb
â”‚   â”‚   â”œâ”€â”€ 05_scoring_and_ranking.ipynb
â”‚   â”‚   â””â”€â”€ 06_statistical_validation.ipynb
â”‚   â”œâ”€â”€ models/                   # Saved ML models
â”‚   â”œâ”€â”€ src/                      # Python scripts
â”‚   â”‚   â”œâ”€â”€ generate_synthetic_data.py
â”‚   â”‚   â””â”€â”€ download_dataset.py
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â””â”€â”€ setup.sh                  # Automated setup script
â”œâ”€â”€ backend/                       # (Future) FastAPI endpoints
â””â”€â”€ frontend/                      # (Future) Next.js UI
```

---

## Tech Stack

### Data Science & ML
- **Python 3.9+**: Core language
- **pandas, NumPy**: Data manipulation
- **scikit-learn**: Machine learning (K-means, PCA, preprocessing)
- **spaCy**: NLP for skill extraction
- **matplotlib, seaborn**: Visualizations
- **SciPy**: Statistical tests

### Data
- **800+ resumes**: Synthetic + optional real data
- **150+ skills**: Across 9 categories
- **10 job categories**: Data Science, Web Dev, Mobile Dev, etc.

### Future (To Be Implemented)
- **Backend**: FastAPI (Python) for ML endpoints
- **Frontend**: Next.js + React + TailwindCSS
- **Deployment**: Vercel (frontend) + Railway/Render (backend)

---

## Data Science Techniques Demonstrated

### âœ… Implemented & Validated

1. **Data Wrangling**
   - Missing value handling (imputation, removal)
   - Duplicate detection & removal
   - Text cleaning & normalization
   - Outlier detection (IQR method)

2. **Feature Engineering**
   - NLP-based skill extraction (150+ skills)
   - Derived features (skill diversity, technical ratio)
   - Feature encoding (label, one-hot)
   - Feature scaling (standardization)

3. **Unsupervised Learning**
   - K-means clustering (optimal K via elbow + silhouette)
   - Hierarchical clustering (Ward linkage)
   - Cluster validation (multiple metrics)
   - Cluster characterization & naming

4. **Statistical Analysis**
   - Distribution analysis (normality tests)
   - Hypothesis testing (4 tests with p-values)
   - Correlation analysis
   - Feature importance analysis

5. **Dimensionality Reduction**
   - PCA (Principal Component Analysis)
   - t-SNE (t-Distributed Stochastic Neighbor Embedding)

6. **Algorithm Design**
   - Multi-factor weighted scoring
   - Percentile ranking system
   - Job-resume matching (TF-IDF + cosine similarity)

7. **Model Validation**
   - Silhouette score
   - Davies-Bouldin index
   - Calinski-Harabasz index
   - Adjusted Rand Index (ARI)
   - Normalized Mutual Information (NMI)

---

## Key Results

- **Clustering**: Identified optimal K clusters with silhouette score ~0.4-0.6
- **Scoring**: Multi-dimensional scoring (0-100) with good distribution
- **Statistical Validation**: 4 hypothesis tests conducted (all documented)
- **Skill Extraction**: 150+ skills successfully extracted via NLP
- **Feature Engineering**: 20+ engineered features from raw data

---

## Documentation

- **[README.md](README.md)** - This file (quick start guide)
- **[docs/01_PROJECT_OVERVIEW.md](docs/01_PROJECT_OVERVIEW.md)** - Project overview & architecture
- **[docs/02_ML_METHODOLOGY.md](docs/02_ML_METHODOLOGY.md)** - In-depth FDS concepts & methodology
- **[docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - Common issues & solutions

---

## Troubleshooting

### Common Issues

**1. Module Not Found Error**
```bash
# Make sure virtual environment is activated
source venv/bin/activate
pip install -r requirements.txt
```

**2. spaCy Model Missing**
```bash
python -m spacy download en_core_web_sm
```

**3. Jupyter Not Found**
```bash
pip install jupyter ipykernel
```

**4. File Not Found in Notebooks**
- Run notebooks in order (01 â†’ 06)
- Each notebook creates outputs needed by the next

**5. JSON Serialization Error**
- Already fixed in Notebook 04 (int32 â†’ string conversion)
- Re-download the latest version if you see this

See [TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) for more solutions.

---

## Innovation & Impact

### What Makes This Different

Traditional ATS systems provide:
- âŒ Generic rejections ("not a good fit")
- âŒ No feedback for improvement
- âŒ Black-box decision making
- âŒ No transparency

Our ATS provides:
- âœ… **Percentile feedback**: "You're in the top 35% of candidates"
- âœ… **Skill gap analysis**: "You're missing: React, Docker"
- âœ… **Transparent scoring**: See exact score breakdown
- âœ… **Improvement recommendations**: Actionable next steps
- âœ… **Data-driven clustering**: Scientific candidate grouping

---

## Project Timeline

- **Week 1** âœ…: Data Foundation (wrangling, preprocessing, feature engineering)
- **Week 2** âœ…: ML Models (clustering, scoring, statistical validation)
- **Week 3** ğŸ”„: Backend API (FastAPI endpoints)
- **Week 4** â³: Frontend (Next.js dashboards)

**Current Status**: ML pipeline complete and validated

---

## Contributing

This is an academic project for a Foundations of Data Science course.

**Team**: [Your Name]
**Course**: Foundations of Data Science
**Institution**: [Your University]

---

## License

Educational use only.

---

## Acknowledgments

- **Kaggle** for resume datasets
- **scikit-learn** for ML algorithms
- **spaCy** for NLP capabilities
- Course instructors for guidance

---

## Contact

For questions about this project:
- See documentation in `docs/` folder
- Check [TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)

---

**Ready to explore?** Run the notebooks in order and see the ML pipeline in action! ğŸš€
